{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: numpy in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from scikit-optimize) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from scikit-optimize) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from scikit-optimize) (0.20.3)\n",
      "Requirement already satisfied: hyperopt in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (0.1.2)\n",
      "Requirement already satisfied: future in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (0.17.1)\n",
      "Requirement already satisfied: pymongo in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (3.9.0)\n",
      "Requirement already satisfied: scipy in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (1.2.1)\n",
      "Requirement already satisfied: tqdm in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (4.31.1)\n",
      "Requirement already satisfied: six in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (1.12.0)\n",
      "Requirement already satisfied: numpy in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (1.16.2)\n",
      "Requirement already satisfied: networkx in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (2.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from networkx->hyperopt) (4.4.0)\n",
      "Requirement already satisfied: keras in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (2.2.4)\n",
      "Requirement already satisfied: h5py in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from keras) (1.2.1)\n",
      "Requirement already satisfied: pyyaml in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from keras) (5.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from keras) (1.16.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: tensorflow in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (1.11.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (0.33.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (0.1.7)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (1.22.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (3.9.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorflow) (1.16.2)\n",
      "Requirement already satisfied: h5py in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (41.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (2.9.0)\n",
      "Requirement already satisfied: six in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from h5py) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from h5py) (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import math\n",
    "from sklearn.base import clone\n",
    "!pip install scikit-optimize\n",
    "from skopt import gp_minimize\n",
    "!pip install hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt import space_eval\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "!pip install h5py\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calendar_dfs = []\n",
    "# for file_name in glob.glob('./data/amsterdam/calendar*.gz'):\n",
    "#     calendar_dfs.append(pd.read_csv(file_name, compression='gzip'))\n",
    "# calendar = pd.concat(calendar_dfs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_dfs = []\n",
    "# for file_name in glob.glob('./data/amsterdam/reviews*.gz'):\n",
    "#     reviews_dfs.append(pd.read_csv(file_name, compression='gzip'))\n",
    "# reviews = pd.concat(reviews_dfs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listings_dfs = []\n",
    "# for file_name in glob.glob('./data/amsterdam/listings*.gz'):\n",
    "#     listings_dfs.append(pd.read_csv(file_name, compression='gzip', dtype=\"str\"))\n",
    "# listings = pd.concat(listings_dfs, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(\"./data/boston/calendar.csv\")\n",
    "reviews = pd.read_csv(\"./data/boston/reviews.csv\")\n",
    "listings = pd.read_csv(\"./data/boston/listings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming 'price' from string to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    250.0\n",
       "1     65.0\n",
       "2     65.0\n",
       "3     75.0\n",
       "4     79.0\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Converts price in the string format like \"$1,125.00\" into numeric value 1125.00\n",
    "    INPUT:\n",
    "    - string price in string format\n",
    "    OUTPUT:\n",
    "    - float value corresponding to the price or None if the input is not parseable to float\n",
    "\"\"\"\n",
    "def str_to_num (string):\n",
    "    if string is not None:\n",
    "        if type(string) is str and string.startswith('$'):\n",
    "            return float(string.replace('$', '').replace(',', ''))\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "listings_cleaned = pd.concat([listings.drop('price', axis=1), listings[\"price\"].apply(str_to_num)], axis=1)\n",
    "listings_cleaned['price'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   NaN\n",
       "1   NaN\n",
       "2   NaN\n",
       "3   NaN\n",
       "4   NaN\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_cleaned = pd.concat([calendar.drop('price', axis=1), calendar[\"price\"].apply(str_to_num)], axis=1)\n",
    "calendar_cleaned['price'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting 'avilable' field from string to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: available, dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_cleaned['available'] = calendar_cleaned['available'] == 't'\n",
    "calendar_cleaned['available'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting 'date' from string to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_cleaned[\"date\"] = pd.to_datetime(calendar_cleaned[\"date\"], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting location field from numeric to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_cleaned[\"location_categorical\"] = listings_cleaned.apply(lambda row: str((format(row.latitude, '.2f'), format(row.longitude, '.2f'))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ('42.28', '-71.13')\n",
       "1    ('42.29', '-71.13')\n",
       "2    ('42.29', '-71.14')\n",
       "3    ('42.28', '-71.12')\n",
       "4    ('42.28', '-71.14')\n",
       "Name: location_categorical, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_cleaned.location_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting 'amenities' field containing list into separate categorical columns for each amenity in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TV',\n",
       " 'Wireless Internet',\n",
       " 'Kitchen',\n",
       " 'Free Parking on Premises',\n",
       " 'Pets live on this property',\n",
       " 'Dog(s)',\n",
       " 'Heating',\n",
       " 'Family/Kid Friendly',\n",
       " 'Washer',\n",
       " 'Dryer',\n",
       " 'Smoke Detector',\n",
       " 'Fire Extinguisher',\n",
       " 'Essentials',\n",
       " 'Shampoo',\n",
       " 'Laptop Friendly Workspace']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda amenity : amenity.replace(\"\\\"\",\"\").replace(\"{\",\"\").replace(\"}\", \"\"), listings_cleaned.amenities.iloc[0].split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Separates the string value of `amenities` attribute in the row, into a list of individual amenities.\n",
    "    \n",
    "    INPUT:\n",
    "    - row : from dataset having amenities attribute\n",
    "    OUTPUT:    \n",
    "    - list of amenities derived from the value of `amenities` attribute in row\n",
    "\"\"\"\n",
    "def separate_amenities(row):\n",
    "    amenities = row.amenities\n",
    "    list_to_return = []\n",
    "    if (amenities is not None and type(amenities) == str):            \n",
    "        list_to_return = list(map(lambda amenity : amenity.replace(\"\\\"\",\"\").replace(\"{\",\"\").replace(\"}\", \"\"), amenities.split(\",\")))\n",
    "    if '' in list_to_return:\n",
    "        list_to_return.remove('')\n",
    "    return list_to_return\n",
    "listings_cleaned[\"amenities_list\"] = listings_cleaned.apply(separate_amenities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TV', 'Wireless Internet', 'Kitchen', 'Free Parking on Premises',\n",
       "       'Pets live on this property', 'Dog(s)', 'Heating',\n",
       "       'Family/Kid Friendly', 'Washer', 'Dryer', 'Smoke Detector',\n",
       "       'Fire Extinguisher', 'Essentials', 'Shampoo',\n",
       "       'Laptop Friendly Workspace', 'Internet', 'Air Conditioning',\n",
       "       'Pets Allowed', 'Carbon Monoxide Detector', 'Lock on Bedroom Door',\n",
       "       'Hangers', 'Hair Dryer', 'Iron', 'Cable TV', 'First Aid Kit',\n",
       "       'Safety Card', 'translation missing: en.hosting_amenity_49',\n",
       "       'translation missing: en.hosting_amenity_50', 'Gym', 'Breakfast',\n",
       "       'Indoor Fireplace', 'Cat(s)', '24-Hour Check-in', 'Hot Tub',\n",
       "       'Buzzer/Wireless Intercom', 'Other pet(s)', 'Washer / Dryer',\n",
       "       'Smoking Allowed', 'Suitable for Events', 'Wheelchair Accessible',\n",
       "       'Elevator in Building', 'Pool', 'Doorman',\n",
       "       'Paid Parking Off Premises', 'Free Parking on Street'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_amenities = listings_cleaned['amenities_list'].apply(pd.Series).stack().unique()\n",
    "possible_amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Assigns new boolean attribute to the row based on the presence of that amenity in the list.\n",
    "    Returns updated row with additional attributes corresponding to the amenities added to it.\n",
    "    \n",
    "    INPUT:\n",
    "    - row containing attributes related to the property, including `amenities_list`\n",
    "    OUTPUT:\n",
    "    - row containing newly added boolean attributes indicating the presence of each possible type of amenity in the property\n",
    "\"\"\"\n",
    "def add_amenities_columns (row):\n",
    "    amenities = set(row.amenities_list)\n",
    "    for possible_amenity in possible_amenities:\n",
    "        row[\"amenity_\" + possible_amenity] = possible_amenity in amenities\n",
    "    return row\n",
    "\n",
    "listings_cleaned = listings_cleaned.apply(add_amenities_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the activation date for each property listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3353</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>True</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3353</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3353</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>True</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3353</td>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5506</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-10-10</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-28</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-25</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-19</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-18</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-15</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-08</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6695</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>True</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6695</td>\n",
       "      <td>2016-10-24</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6695</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>True</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    listing_id       date  available  price\n",
       "0         3353 2017-09-05       True   36.0\n",
       "1         3353 2016-12-30      False    NaN\n",
       "2         3353 2017-08-18       True   36.0\n",
       "3         3353 2016-10-12      False    NaN\n",
       "4         5506 2017-09-05       True  145.0\n",
       "5         5506 2016-10-10      False    NaN\n",
       "6         5506 2016-10-03       True  145.0\n",
       "7         5506 2016-09-30      False    NaN\n",
       "8         5506 2016-09-28       True  145.0\n",
       "9         5506 2016-09-25      False    NaN\n",
       "10        5506 2016-09-22       True  145.0\n",
       "11        5506 2016-09-21      False    NaN\n",
       "12        5506 2016-09-19       True  145.0\n",
       "13        5506 2016-09-18      False    NaN\n",
       "14        5506 2016-09-15       True  145.0\n",
       "15        5506 2016-09-12      False    NaN\n",
       "16        5506 2016-09-08       True  145.0\n",
       "17        6695 2017-09-05       True  195.0\n",
       "18        6695 2016-10-24      False    NaN\n",
       "19        6695 2016-10-19       True  195.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = calendar_cleaned\n",
    "df = df.groupby('listing_id', group_keys=False)\\\n",
    "    .apply(lambda x: x[x.available.ne(x.available.shift())])\\\n",
    "    .reset_index(drop=True)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_activation_dates = df[df.available == True].groupby(\"listing_id\")[[\"listing_id\",\"date\"]].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>530983</th>\n",
       "      <td>530983</td>\n",
       "      <td>2016-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815639</th>\n",
       "      <td>815639</td>\n",
       "      <td>2016-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12726343</th>\n",
       "      <td>12726343</td>\n",
       "      <td>2016-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776391</th>\n",
       "      <td>2776391</td>\n",
       "      <td>2016-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10524612</th>\n",
       "      <td>10524612</td>\n",
       "      <td>2016-09-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            listing_id       date\n",
       "listing_id                       \n",
       "530983          530983 2016-09-06\n",
       "815639          815639 2016-09-06\n",
       "12726343      12726343 2016-09-06\n",
       "2776391        2776391 2016-09-06\n",
       "10524612      10524612 2016-09-06"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listing_activation_dates.sort_values(by=[\"date\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only retaining entries in calendar for each property after its activation date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_by_listing_groups = calendar_cleaned.groupby([\"listing_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-03</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>3075044</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>3075044</td>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>3075044</td>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>3075044</td>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>True</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>3075044</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>True</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>6976</td>\n",
       "      <td>2017-05-12</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>6976</td>\n",
       "      <td>2017-05-11</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>6976</td>\n",
       "      <td>2017-05-10</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>6976</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>6976</td>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>1436513</td>\n",
       "      <td>2017-05-10</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>1436513</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>1436513</td>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>1436513</td>\n",
       "      <td>2017-05-07</td>\n",
       "      <td>True</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>1436513</td>\n",
       "      <td>2017-05-06</td>\n",
       "      <td>True</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>7651065</td>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>True</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>7651065</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>True</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>7651065</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>True</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>7651065</td>\n",
       "      <td>2017-06-18</td>\n",
       "      <td>True</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>7651065</td>\n",
       "      <td>2017-06-17</td>\n",
       "      <td>True</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>12386020</td>\n",
       "      <td>2017-04-25</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>12386020</td>\n",
       "      <td>2017-04-24</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>12386020</td>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>12386020</td>\n",
       "      <td>2017-04-22</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>12386020</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306700</th>\n",
       "      <td>14852179</td>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306701</th>\n",
       "      <td>14852179</td>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306702</th>\n",
       "      <td>14852179</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306703</th>\n",
       "      <td>14852179</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306704</th>\n",
       "      <td>14852179</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307065</th>\n",
       "      <td>8373729</td>\n",
       "      <td>2017-05-18</td>\n",
       "      <td>True</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307066</th>\n",
       "      <td>8373729</td>\n",
       "      <td>2017-05-17</td>\n",
       "      <td>True</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307067</th>\n",
       "      <td>8373729</td>\n",
       "      <td>2017-05-16</td>\n",
       "      <td>True</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307068</th>\n",
       "      <td>8373729</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>True</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307069</th>\n",
       "      <td>8373729</td>\n",
       "      <td>2017-05-14</td>\n",
       "      <td>True</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307430</th>\n",
       "      <td>14844274</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>True</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307431</th>\n",
       "      <td>14844274</td>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>True</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307432</th>\n",
       "      <td>14844274</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>True</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307433</th>\n",
       "      <td>14844274</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>True</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307434</th>\n",
       "      <td>14844274</td>\n",
       "      <td>2017-06-18</td>\n",
       "      <td>True</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307795</th>\n",
       "      <td>14585486</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307796</th>\n",
       "      <td>14585486</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307797</th>\n",
       "      <td>14585486</td>\n",
       "      <td>2017-09-03</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307798</th>\n",
       "      <td>14585486</td>\n",
       "      <td>2017-09-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307799</th>\n",
       "      <td>14585486</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308160</th>\n",
       "      <td>14603878</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>True</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308161</th>\n",
       "      <td>14603878</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>True</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308162</th>\n",
       "      <td>14603878</td>\n",
       "      <td>2017-09-03</td>\n",
       "      <td>True</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308163</th>\n",
       "      <td>14603878</td>\n",
       "      <td>2017-09-02</td>\n",
       "      <td>True</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308164</th>\n",
       "      <td>14603878</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>True</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308525</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308526</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308527</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308528</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2017-06-18</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308529</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2017-06-17</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17925 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         listing_id       date  available  price\n",
       "0          12147973 2017-09-05      False    NaN\n",
       "1          12147973 2017-09-04      False    NaN\n",
       "2          12147973 2017-09-03      False    NaN\n",
       "3          12147973 2017-09-02      False    NaN\n",
       "4          12147973 2017-09-01      False    NaN\n",
       "365         3075044 2017-08-22       True   65.0\n",
       "366         3075044 2017-08-21       True   65.0\n",
       "367         3075044 2017-08-20       True   65.0\n",
       "368         3075044 2017-08-19       True   75.0\n",
       "369         3075044 2017-08-18       True   75.0\n",
       "730            6976 2017-05-12       True   65.0\n",
       "731            6976 2017-05-11       True   65.0\n",
       "732            6976 2017-05-10       True   65.0\n",
       "733            6976 2017-05-09       True   65.0\n",
       "734            6976 2017-05-08       True   65.0\n",
       "1095        1436513 2017-05-10      False    NaN\n",
       "1096        1436513 2017-05-09      False    NaN\n",
       "1097        1436513 2017-05-08      False    NaN\n",
       "1098        1436513 2017-05-07       True   75.0\n",
       "1099        1436513 2017-05-06       True   75.0\n",
       "1460        7651065 2017-06-21       True   79.0\n",
       "1461        7651065 2017-06-20       True   79.0\n",
       "1462        7651065 2017-06-19       True   79.0\n",
       "1463        7651065 2017-06-18       True   79.0\n",
       "1464        7651065 2017-06-17       True   79.0\n",
       "1825       12386020 2017-04-25      False    NaN\n",
       "1826       12386020 2017-04-24      False    NaN\n",
       "1827       12386020 2017-04-23      False    NaN\n",
       "1828       12386020 2017-04-22      False    NaN\n",
       "1829       12386020 2017-04-21      False    NaN\n",
       "...             ...        ...        ...    ...\n",
       "1306700    14852179 2017-08-20      False    NaN\n",
       "1306701    14852179 2017-08-19      False    NaN\n",
       "1306702    14852179 2017-08-18      False    NaN\n",
       "1306703    14852179 2017-08-17      False    NaN\n",
       "1306704    14852179 2017-08-16      False    NaN\n",
       "1307065     8373729 2017-05-18       True   69.0\n",
       "1307066     8373729 2017-05-17       True   69.0\n",
       "1307067     8373729 2017-05-16       True   69.0\n",
       "1307068     8373729 2017-05-15       True   69.0\n",
       "1307069     8373729 2017-05-14       True   69.0\n",
       "1307430    14844274 2017-06-22       True  150.0\n",
       "1307431    14844274 2017-06-21       True  150.0\n",
       "1307432    14844274 2017-06-20       True  150.0\n",
       "1307433    14844274 2017-06-19       True  150.0\n",
       "1307434    14844274 2017-06-18       True  150.0\n",
       "1307795    14585486 2017-09-05      False    NaN\n",
       "1307796    14585486 2017-09-04      False    NaN\n",
       "1307797    14585486 2017-09-03      False    NaN\n",
       "1307798    14585486 2017-09-02      False    NaN\n",
       "1307799    14585486 2017-09-01      False    NaN\n",
       "1308160    14603878 2017-09-05       True   59.0\n",
       "1308161    14603878 2017-09-04       True   59.0\n",
       "1308162    14603878 2017-09-03       True   59.0\n",
       "1308163    14603878 2017-09-02       True   59.0\n",
       "1308164    14603878 2017-09-01       True   59.0\n",
       "1308525    14504422 2017-06-21      False    NaN\n",
       "1308526    14504422 2017-06-20      False    NaN\n",
       "1308527    14504422 2017-06-19      False    NaN\n",
       "1308528    14504422 2017-06-18      False    NaN\n",
       "1308529    14504422 2017-06-17      False    NaN\n",
       "\n",
       "[17925 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_by_listing_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_entries_after_activation_date(g):\n",
    "\n",
    "    listing_id = g.name\n",
    "    activation_date_df = listing_activation_dates.query(\"listing_id == @listing_id\")[\"date\"]\n",
    "    if activation_date_df.shape[0] > 0:        \n",
    "        activation_date = activation_date_df.iloc[0]\n",
    "#         print(\"activatation_date: \" + str(activation_date))\n",
    "#         print(g[\"date\"].dtype)\n",
    "#         print(type(activation_date))\n",
    "        return g[g[\"date\"] <= activation_date]\n",
    "    \n",
    "    \n",
    "cal_after_activation_dates =cal_by_listing_groups.apply(select_entries_after_activation_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and add necessary variables to be used for predicting occupancy of property at given time of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars_from_listings = [\"neighbourhood_cleansed\", \n",
    "#                 \"neighbourhood_group_cleansed\",\n",
    "                \"city\",\n",
    "                \"state\",\n",
    "                \"zipcode\",\n",
    "                \"market\",\n",
    "                \"location_categorical\",\n",
    "                \"property_type\",\n",
    "                \"room_type\",\n",
    "                \"accommodates\",\n",
    "                \"bathrooms\",\n",
    "                \"bedrooms\",\n",
    "                \"beds\",\n",
    "                \"bed_type\",\n",
    "                \"square_feet\",\n",
    "                \"guests_included\",\n",
    "                \"minimum_nights\",\n",
    "                \"maximum_nights\",\n",
    "                \"review_scores_rating\",\n",
    "                \"review_scores_accuracy\",\n",
    "                  \"review_scores_cleanliness\",\n",
    "                  \"review_scores_checkin\",\n",
    "                  \"review_scores_communication\",\n",
    "                  \"review_scores_location\",\n",
    "                  \"review_scores_value\",\n",
    "#                   \"jurisdiction_names\",\n",
    "                  \"cancellation_policy\",\n",
    "#                   \"reviews_per_month\",\n",
    "                  \"number_of_reviews\"\n",
    "       ]\n",
    "amenity_variables = list(map(lambda amenity : \"amenity_\" + amenity, possible_amenities))\n",
    "input_vars_from_listings.extend(amenity_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_cleaned[\"month\"] = calendar_cleaned['date'].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_cleaned[\"day_of_week\"] = calendar_cleaned['date'].dt.weekday_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_cleaned[\"week_of_month\"] = np.ceil(calendar_cleaned['date'].dt.day/7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 4, 3, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_cleaned[\"week_of_month\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(listings_cleaned, calendar_cleaned, left_on=\"id\", right_on=\"listing_id\", how=\"inner\", suffixes=(\"_listings\", \"_calendar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df.groupby([\"listing_id\", \"month\", \"day_of_week\"]).apply(lambda x: x.sort_values(by=[\"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.loc[:, \"price_calendar\"] = g[\"price_calendar\"].fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = g.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df.groupby([\"listing_id\"]).apply(lambda x: x.sort_values(by=[\"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.loc[:, \"price_calendar\"] = g[\"price_calendar\"].fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = g.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars_from_calendar = [\"month\", \"day_of_week\", \"week_of_month\", \"price_calendar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_vars = input_vars_from_listings + input_vars_from_calendar;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"available\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a sample of properties to be used as an environment of alternatives competing with the property to be booked that can have impact on whether the given property gets booked or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df[\"date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_ids_environment = list(df.query(\"date == @start_date\").drop_duplicates(['listing_id'])[\"listing_id\"].sample(100, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_env = df[all_input_vars + [\"listing_id\", \"date\"]].query(\"listing_id not in @listing_ids_environment\")\n",
    "for listing_id in listing_ids_environment:\n",
    "    listing_price_availability = df.query(\"listing_id == @listing_id\")[[\"price_calendar\", \"available\", \"date\"]]\n",
    "    X_with_env = pd.merge(X_with_env, listing_price_availability, left_on=\"date\", right_on=\"date\", how=\"inner\", suffixes=(\"\", \"_\"+str(listing_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(X_with_env.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_with_env.sort_values(by=[\"listing_id\", \"date\"]).drop(columns=[\"date\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"available\",\"listing_id\", \"date\"]].query(\"listing_id not in @listing_ids_environment\").sort_values(by=[\"listing_id\", \"date\"]).drop(columns=[\"date\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X[\"listing_id\"] == y[\"listing_id\"]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing values in numeric columns with mean of the corresponding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = X.select_dtypes(exclude=['object']).copy().columns\n",
    "def fill_mean (col):\n",
    "    return col.fillna(col.mean())\n",
    "X.loc[X.index, num_vars] = X[num_vars].apply(fill_mean, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical variables into numeric variables with separate column for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = X.select_dtypes(include=['object']).copy().columns\n",
    "for var in  cat_vars:\n",
    "    # for each cat add dummy var, drop original column\n",
    "    X = pd.concat([X.drop(var, axis=1), pd.get_dummies(X[var], prefix=var, prefix_sep='_', drop_first=False, dummy_na=True)], axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into training and test sets while making sure that no property listing is common between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_listing_ids = X[\"listing_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "listing_ids_train = np.random.choice(np.array(unique_listing_ids), size= int(0.70 * len(unique_listing_ids)), replace=False)\n",
    "listing_ids_test = [l for l in unique_listing_ids if l not in listing_ids_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.query(\"listing_id in @listing_ids_train\")\n",
    "y_train = y.query(\"listing_id in @listing_ids_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.index == y_train.index).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using here only a smaller sample of training data to choose hyper parameters using cross validation technique to complete the experiments in timely manner. However, in real world scenario with availability of time, higher computational resources and memory, entire training dataset should be used for choosing optimal hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = X_train.sample(10000, random_state=42)\n",
    "y_train_sample = y_train.loc[X_train_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_sample[\"listing_id\"] == y_train_sample[\"listing_id\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X.query(\"listing_id in @listing_ids_test\").drop(columns=[\"listing_id\"])\n",
    "y_test = y.query(\"listing_id in @listing_ids_test\").drop(columns=[\"listing_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample.to_csv(\"X_train_sample.csv\", index=False)\n",
    "y_train_sample.to_csv(\"y_train_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"X_test.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[\"available\"].value_counts()/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the ratio of samples classified for output True and False, the data looks to be balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing cross validation splits to into training and test sets while making sure that no property listing is common between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGridSearchCV(object):\n",
    "    \n",
    "    def __init__(self, estimator, param_grid, cv=3):\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.best_score = -math.inf\n",
    "        self.best_estimator = None\n",
    "        self.best_estimator_scores_mean = -math.inf\n",
    "        self.best_estimator_scores_stdev = -math.inf\n",
    "        self.best_params= None\n",
    "        self.cv = cv       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for param_point in self.get_param_grid_points():\n",
    "            def splits(array, k):\n",
    "                splits = []\n",
    "                \n",
    "                for i in range(k):\n",
    "                    split_length = math.ceil(len(array)/k)\n",
    "                    split = []\n",
    "                    for j in range(i * split_length, (i+1) * split_length):\n",
    "                        if j < len(array):\n",
    "                            split.append(array[j])\n",
    "                        \n",
    "                    splits.append(split)\n",
    "                cvs = []\n",
    "                for i, split in enumerate(splits):\n",
    "                    train = []\n",
    "                    for j, other_split in enumerate(splits):\n",
    "                        if i != j:\n",
    "                            train += other_split\n",
    "                    test = [] + split\n",
    "                    cvs.append((train, test))\n",
    "                return cvs\n",
    "                    \n",
    "            scores_for_point = []\n",
    "            estimator = clone(self.estimator)\n",
    "            estimator.set_params(**param_point)\n",
    "            for l_train, l_test in splits(list(X[\"listing_id\"].unique()), k=self.cv):                \n",
    "                X_train = X.query(\"listing_id in @l_train\").drop(columns=[\"listing_id\"])\n",
    "                X_test = X.query(\"listing_id in @l_test\").drop(columns=[\"listing_id\"])\n",
    "                y_train = y.query(\"listing_id in @l_train\").drop(columns=[\"listing_id\"])\n",
    "                y_test = y.query(\"listing_id in @l_test\").drop(columns=[\"listing_id\"])                \n",
    "                estimator.fit(X_train, y_train.values.ravel())\n",
    "                y_preds = estimator.predict(X_test)\n",
    "                f1_score_1 = f1_score(y_test, y_preds, pos_label=True)\n",
    "                f1_score_0 = f1_score(y_test, y_preds, pos_label=False)\n",
    "                score = min([f1_score_1, f1_score_0])\n",
    "                scores_for_point.append(score)\n",
    "                \n",
    "            mean_score = np.mean(scores_for_point)\n",
    "            if mean_score > self.best_estimator_scores_mean:\n",
    "                self.best_estimator_scores_mean = mean_score\n",
    "                self.best_estimator_scores_stdev = np.std(scores_for_point)\n",
    "                self.best_estimator = estimator\n",
    "                self.best_params = self.best_estimator.get_params()\n",
    "                self.best_score = self.best_estimator_scores_mean\n",
    "   \n",
    "            \n",
    "    def get_param_grid_points(self):\n",
    "        points = [{}]\n",
    "        for param, values in self.param_grid.items():\n",
    "            new_points = []\n",
    "            for point in points:\n",
    "                for value in values:                    \n",
    "                    new_point = point.copy()\n",
    "                    new_point[param]= value\n",
    "                    new_points.append(new_point)\n",
    "            points = new_points\n",
    "        return points\n",
    "    \n",
    "    def print_best(self):\n",
    "        print(\"Best score:\")\n",
    "        print(self.best_score)\n",
    "        print(\"Best params:\")\n",
    "        print(self.best_params)\n",
    "        print(\"Best estimator scores mean:\")\n",
    "        print(self.best_estimator_scores_mean)\n",
    "        print(\"Best estimator scores stdev:\")\n",
    "        print(self.best_estimator_scores_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cross_val_score(estimator, X, y):\n",
    "    def splits(array, k):\n",
    "        splits = []\n",
    "\n",
    "        for i in range(k):\n",
    "            split_length = math.ceil(len(array)/k)\n",
    "            split = []\n",
    "            for j in range(i * split_length, (i+1) * split_length):\n",
    "                if j < len(array):\n",
    "                    split.append(array[j])\n",
    "\n",
    "            splits.append(split)\n",
    "        cvs = []\n",
    "        for i, split in enumerate(splits):\n",
    "            train = []\n",
    "            for j, other_split in enumerate(splits):\n",
    "                if i != j:\n",
    "                    train += other_split\n",
    "            test = [] + split\n",
    "            cvs.append((train, test))\n",
    "        return cvs\n",
    "\n",
    "    scores_for_point = []\n",
    "    for l_train, l_test in splits(list(X[\"listing_id\"].unique()), k=5):\n",
    "        X_train = X.query(\"listing_id in @l_train\").drop(columns=[\"listing_id\"])\n",
    "        y_train = y.query(\"listing_id in @l_train\").drop(columns=[\"listing_id\"])\n",
    "        X_test = X.query(\"listing_id in @l_test\").drop(columns=[\"listing_id\"])\n",
    "        y_test = y.query(\"listing_id in @l_test\").drop(columns=[\"listing_id\"])\n",
    "        estimator.fit(X=X_train, y=y_train.astype('int').values.ravel())\n",
    "        y_preds = estimator.predict(X_test)\n",
    "        f1_score_1 = f1_score(y_test, y_preds, pos_label=True)\n",
    "        f1_score_0 = f1_score(y_test, y_preds, pos_label=False)\n",
    "        score = min([f1_score_1, f1_score_0])\n",
    "        scores_for_point.append(score)\n",
    "\n",
    "    mean_score = np.mean(scores_for_point)\n",
    "    return mean_score\n",
    "\n",
    "import decimal\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    start = decimal.Decimal(start)\n",
    "    stop = decimal.Decimal(stop)\n",
    "    step = decimal.Decimal(step)\n",
    "    while (start < stop):\n",
    "        yield float(start)\n",
    "        start += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using Tree of Parzen Estimators algorithm implemented in Hyperopt library for choosing hyper parameters to be evaluated to find the optimal ones that maximizes the score = min(f1_score_for_positive_class, f1_score_for_negative_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_dtc(params):\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.set_params(**params)    \n",
    "    score = my_cross_val_score(clf, X=X_train_sample, y=y_train_sample)\n",
    "    print(params)\n",
    "    print(\"score: \" + str(score))\n",
    "    return -score\n",
    "\n",
    "dtc_param_space = {\n",
    "    \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", range(1,6,2)),\n",
    "    \"min_samples_split\": hp.choice(\"min_samples_split\", [2*x for x in range(1,11,2)]),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", [2**x for x in range(2,11)]),    \n",
    "    \"random_state\": hp.choice(\"random_state\", [42])\n",
    "}\n",
    "start_time = time.time()\n",
    "best_params_dtc = space_eval(ada_param_space, fmin(objective_dtc, dtc_param_space, algo=tpe.suggest, max_evals=100))\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))\n",
    "print (\"Best params: \")\n",
    "print (best_params_dtc)\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.set_params(**best_params_dtc)\n",
    "dtc.fit(X=X_train.drop(columns=[\"listing_id\"]), y=y_train.drop(columns=[\"listing_id\"]))\n",
    "pickle.dump(dtc, open('dtc_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bayesian Optimization technique implemented in gp_minimize method of Scikit Optimize package to choose optimal hyper parameters that maximizes the score = min(f1_score_for_positive_class, f1_score_for_negative_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Bayesian Optimization technique implemented in \n",
    "# gp_minimize method of Scikit Optimize package \n",
    "# to choose optimal hyper parameters.\n",
    "\n",
    "# def objective_dtc(params):\n",
    "#     clf = DecisionTreeClassifier(\n",
    "#         max_depth = params[0],\n",
    "#         min_samples_leaf = params[1],\n",
    "#         max_features = params[2],\n",
    "#         min_samples_split = params[3],\n",
    "#         random_state = 42\n",
    "#     )\n",
    "#     score = my_cross_val_score(clf, X=X_train_sample, y=y_train_sample)\n",
    "#     return -score\n",
    "# start_time = time.time()\n",
    "# dtc_opt_result = gp_minimize(\n",
    "#     func=objective_dtc,\n",
    "#     dimensions=[\n",
    "#         (5, 30),\n",
    "#         (1, 10),\n",
    "#         (0.1, 1.0),\n",
    "#         (2, 20)\n",
    "#     ],\n",
    "#     random_state=42\n",
    "# )\n",
    "# elapsed_time = (time.time() - start_time) / 60\n",
    "# print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))\n",
    "# best_params_list_dtc = dtc_opt_result.x\n",
    "# best_score_dtc = -dtc_opt_result.fun\n",
    "# best_params_dtc = {\n",
    "#     \"max_depth\": best_params_list_dtc[0],\n",
    "#     \"min_samples_leaf\": best_params_list_dtc[1],\n",
    "#     \"max_features\": best_params_list_dtc[2],\n",
    "#     \"min_samples_split\": best_params_list_dtc[3]\n",
    "# }\n",
    "# print (\"Best score: \" + str(best_score_dtc))\n",
    "# dtc = DecisionTreeClassifier()\n",
    "# best_params_dtc[\"random_state\"] = 42\n",
    "# dtc.set_params(**best_params_dtc)\n",
    "# dtc.fit(X=X_train.drop(columns=[\"listing_id\"]), y=y_train.drop(columns=[\"listing_id\"]))\n",
    "# pickle.dump(dtc, open('dtc_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_ada(params):\n",
    "    clf = AdaBoostClassifier(DecisionTreeClassifier())\n",
    "    clf.set_params(**params)    \n",
    "    score = my_cross_val_score(clf, X=X_train_sample, y=y_train_sample)\n",
    "    print(params)\n",
    "    print(\"score: \" + str(score))\n",
    "    return -score\n",
    "ada_param_space = {\n",
    "    \"base_estimator__min_samples_leaf\": hp.choice(\"base_estimator__min_samples_leaf\", range(1,6,2)),\n",
    "    \"base_estimator__min_samples_split\": hp.choice(\"base_estimator__min_samples_split\", [2*x for x in range(1,11,2)]),\n",
    "    \"base_estimator__max_depth\": hp.choice(\"base_estimator__max_depth\", [2**x for x in range(2,11)]),\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", range(10, 101, 10)),\n",
    "    \"random_state\": hp.choice(\"random_state\", [42])\n",
    "}\n",
    "start_time = time.time()\n",
    "best_params_ada = space_eval(ada_param_space, fmin(objective_ada, ada_param_space, algo=tpe.suggest, max_evals=100))\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))\n",
    "print (\"Best params: \")\n",
    "print (best_params_ada)\n",
    "ada = AdaBoostClassifier(DecisionTreeClassifier())\n",
    "ada.set_params(**best_params_ada)\n",
    "ada.fit(X=X_train.drop(columns=[\"listing_id\"]), y=y_train.drop(columns=[\"listing_id\"]))\n",
    "pickle.dump(ada, open('ada_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(DecisionTreeClassifier())\n",
    "best_params_ada = {'base_estimator__max_depth': 64, 'base_estimator__min_samples_leaf': 3, 'base_estimator__min_samples_split': 6, 'n_estimators': 100, 'random_state': 42}\n",
    "ada.set_params(**best_params_ada)\n",
    "ada.fit(X=X_train.drop(columns=[\"listing_id\"]), y=y_train.drop(columns=[\"listing_id\"]))\n",
    "pickle.dump(ada, open('ada_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_gb(params):\n",
    "    clf = GradientBoostingClassifier()\n",
    "    clf.set_params(**params)    \n",
    "    score = my_cross_val_score(clf, X=X_train_sample, y=y_train_sample)\n",
    "    print(params)\n",
    "    print(\"score: \" + str(score))\n",
    "    return -score\n",
    "\n",
    "gb_param_space = {    \n",
    "    \"max_depth\": hp.choice(\"max_depth\", range(1, 11, 1)),\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", range(10, 101, 10)),\n",
    "    \"random_state\": hp.choice(\"random_state\", [42])\n",
    "}\n",
    "start_time = time.time()\n",
    "best_params_gb = space_eval(gb_param_space, fmin(objective_gb, gb_param_space, algo=tpe.suggest, max_evals=100))\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))\n",
    "print (\"Best params: \")\n",
    "print (best_params_gb)\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.set_params(**best_params_gb)\n",
    "gb.fit(X=X_train.drop(columns=[\"listing_id\"]), y=y_train.drop(columns=[\"listing_id\"]).values.ravel())\n",
    "pickle.dump(gb, open('gb_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf(params):\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.set_params(**params)\n",
    "    score = my_cross_val_score(clf, X=X_train_sample, y=y_train_sample)\n",
    "    print(params)\n",
    "    print(\"score: \" + str(score))\n",
    "    return -score\n",
    "rf_param_space = {\n",
    "    \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", range(1, 10, 2)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", [2**x for x in range(2,11)]),\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", range(10, 101, 10)),\n",
    "    \"random_state\": hp.choice(\"random_state\", [42])\n",
    "}\n",
    "start_time = time.time()\n",
    "best_params_rf = space_eval(rf_param_space, fmin(objective_rf, rf_param_space, algo=tpe.suggest, max_evals=100))\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))\n",
    "print (\"Best params: \")\n",
    "print (best_params_rf)\n",
    "rf = RandomForestClassifier()\n",
    "rf.set_params(**best_params_rf)\n",
    "rf.fit(X=X_train.drop(columns=[\"listing_id\"]), y=y_train.drop(columns=[\"listing_id\"]).values.ravel())\n",
    "pickle.dump(rf, open('rf_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm(params):\n",
    "    clf = SVC()\n",
    "    clf.set_params(**params)\n",
    "    score = my_cross_val_score(clf, X=X_train_sample, y=y_train_sample)\n",
    "    print(params)\n",
    "    print(\"score: \" + str(score))\n",
    "    return -score\n",
    "svm_param_space = {\n",
    "    \"kernel\": hp.choice(\"kernel\", ['rbf']),\n",
    "    \"gamma\": hp.choice(\"gamma\", [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]),\n",
    "    \"C\": hp.choice(\"C\", [0.1, 1, 5, 10, 25, 50, 75, 100, 250, 500, 750, 1000]),\n",
    "    \"random_state\": hp.choice(\"random_state\", [42]),\n",
    "    \"probability\": hp.choice(\"probability\", [True])\n",
    "}\n",
    "start_time = time.time()\n",
    "best_params_svm = space_eval(svm_param_space, fmin(objective_svm, svm_param_space, algo=tpe.suggest, max_evals=100))\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))\n",
    "print (\"Best params: \")\n",
    "print (best_params_svm)\n",
    "svm = SVC()\n",
    "svm.set_params(**best_params_svm)\n",
    "svm.fit(X=X_train.drop(columns=[\"listing_id\"]), y=y_train.drop(columns=[\"listing_id\"]).values.ravel())\n",
    "pickle.dump(svm, open('svm_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(n_features, learn_rate):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(600, input_shape=(n_features,),\n",
    "              kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.45))\n",
    "    \n",
    "    model.add(Dense(300, input_shape=(n_features,),\n",
    "              kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.35))\n",
    "    \n",
    "    model.add(Dense(150, input_shape=(n_features,),\n",
    "              kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(75, kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.15))\n",
    "    \n",
    "    model.add(Dense(35, kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.10))\n",
    "    \n",
    "    model.add(Dense(10, kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.10))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=learn_rate),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = make_model(len(X_train.drop(columns=[\"listing_id\"]).columns), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_listing_ids = list(X_train[[\"listing_id\"]].drop_duplicates([\"listing_id\"]).sample(frac=0.1)[\"listing_id\"])\n",
    "X_val_nn = X_train.query(\"listing_id in @validation_listing_ids\").drop(columns=[\"listing_id\"])\n",
    "y_val_nn = y_train.query(\"listing_id in @validation_listing_ids\").drop(columns=[\"listing_id\"])\n",
    "X_train_nn = X_train.query(\"listing_id not in @validation_listing_ids\").drop(columns=[\"listing_id\"])\n",
    "y_train_nn = y_train.query(\"listing_id not in @validation_listing_ids\").drop(columns=[\"listing_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.fit(X_train_nn, y_train_nn, validation_data=(X_val_nn, y_val_nn), epochs=50, verbose=1,\n",
    "            callbacks=[\n",
    "                EarlyStopping(monitor='val_loss', mode='min', verbose=1),\n",
    "                ModelCheckpoint('nn_best_model.h5', monitor='val_loss', mode='min', verbose=1)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.load_weights('nn_best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = pd.read_csv(\"X_train_sample.csv\")\n",
    "y_train_sample = pd.read_csv(\"y_train_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "def print_top10(feature_names, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    pp.pprint(sorted(list(zip(feature_names, clf.feature_importances_)), key=lambda x: x[1], \n",
    "reverse=True)[:20])\n",
    "\n",
    "for model_name in models.keys():\n",
    "    model_file_name = models[model_name]\n",
    "    model = pickle.load(open(model_file_name, 'rb'))\n",
    "    print(\"\\nModel: \" + model_name)\n",
    "    print(\"Best Params: \")\n",
    "    pp.pprint(model.get_params())\n",
    "    print(\"Top 10 Feature importances: \")\n",
    "    print_top10(X_train_sample.columns, model, model.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction & Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"X_test.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest f1-score: 0.7314898673823129\n",
      "Model: Gradient Boost f1-score: 0.7307059891734311\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Random Forest\": \"rf_best.pkl\",\n",
    "    \"Gradient Boost\": \"gb_best.pkl\",\n",
    "#     \"Ada Boost\": \"ada_best.pkl\",\n",
    "#     \"Single Decision Tree\": \"dtc_best.pkl\"\n",
    "}\n",
    "for model_name in models.keys():\n",
    "    model_file_name = models[model_name]\n",
    "    model = pickle.load(open(model_file_name, 'rb'))\n",
    "    y_preds = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_preds, average=\"macro\")\n",
    "    print (\"Model: \" + model_name + \" f1-score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0904 20:09:31.182117 140736017159040 deprecation_wrapper.py:119] From /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0904 20:09:31.600405 140736017159040 deprecation_wrapper.py:119] From /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0904 20:09:31.881095 140736017159040 deprecation_wrapper.py:119] From /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0904 20:09:31.882040 140736017159040 deprecation_wrapper.py:119] From /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0904 20:09:31.944174 140736017159040 deprecation.py:506] From /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0904 20:09:32.511731 140736017159040 deprecation_wrapper.py:119] From /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0904 20:09:33.843790 140736017159040 deprecation_wrapper.py:119] From /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0904 20:09:33.854117 140736017159040 deprecation.py:323] From /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "nn_model = load_model('nn_best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381790/381790 [==============================] - 51s 134us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6647628992374424, 0.6641871185730376]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.evaluate(x=X_test,y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_nn = nn_model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.6565949692226779\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, y_preds_nn, average=\"macro\")\n",
    "print (\"f1-score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding the booking price of property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I tune the booking price of the property by looking at the probability of the property being booked at given price. If the probability is too low, I try increasing it by setting the price the of that property slightly low and see if it increases the probability. My goal is to maximize the profit of the company. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequential.predict_proba = Sequential.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    start = decimal.Decimal(start)\n",
    "    stop = decimal.Decimal(stop)\n",
    "    step = decimal.Decimal(step)\n",
    "    while (start < stop):\n",
    "        yield float(start)\n",
    "        start += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = list(float_range('0.025', '1', '0.025'))\n",
    "revenue_by_threshold= pd.DataFrame({\"revenue\": 0}, index=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_revenue_at_tuned_price(row, threshold, classifiers):\n",
    "    current_price = row[\"price_calendar\"]\n",
    "#     print(\"price_calendar: \" + str(current_price))\n",
    "    \n",
    "    def calc_average_prob_booked(row):        \n",
    "        df = pd.DataFrame(data=row.to_dict(), index=[0])\n",
    "        sum_prob_booked = 0\n",
    "        for clf in classifiers:\n",
    "            sum_prob_booked += clf.predict_proba(df)[0][0]\n",
    "        return sum_prob_booked/len(classifiers)\n",
    "    patience = 50\n",
    "    prob_booked = calc_average_prob_booked(row)\n",
    "    \n",
    "#     print (\"probability of booked at price_calendar: \" + str(prob_booked))\n",
    "    if (prob_booked < threshold):\n",
    "#         print (\"Will try with reduced the price\")\n",
    "        trial_price = current_price\n",
    "        saved_trial_price = trial_price\n",
    "        steps_in_trap = 0\n",
    "        while (prob_booked < threshold and trial_price > 10):\n",
    "            trial_price = trial_price * 0.99\n",
    "            row[\"price_calendar\"] = trial_price\n",
    "            prob_booked_trial = calc_average_prob_booked(row)\n",
    "#             print (\"trial_price: \" + str(trial_price))\n",
    "#             print (\"prob_booked_trial: \" + str(prob_booked_trial))\n",
    "\n",
    "            if prob_booked_trial >= prob_booked:\n",
    "                if prob_booked_trial == prob_booked:\n",
    "                    steps_in_trap += 1\n",
    "                    if steps_in_trap > patience:\n",
    "                        return saved_trial_price * prob_booked\n",
    "                else:\n",
    "                    saved_trial_price = trial_price\n",
    "                    steps_in_trap = 0\n",
    "                prob_booked = prob_booked_trial\n",
    "            else:\n",
    "                steps_in_trap += 1\n",
    "                if steps_in_trap > patience:\n",
    "                    return saved_trial_price * prob_booked\n",
    "\n",
    "        return trial_price * prob_booked\n",
    "    else:\n",
    "#         print (\"Will try with increased the price\")\n",
    "        trial_price = current_price\n",
    "        saved_trial_price = trial_price\n",
    "        steps_in_trap = 0\n",
    "        while (True):\n",
    "            trial_price = trial_price * 1.01\n",
    "            row[\"price_calendar\"] = trial_price\n",
    "            prob_booked_trial = calc_average_prob_booked(row)\n",
    "#             print (\"trial_price: \" + str(trial_price))\n",
    "#             print (\"prob_booked_trial: \" + str(prob_booked_trial))\n",
    "            if prob_booked_trial <= prob_booked:\n",
    "                if prob_booked_trial == prob_booked:\n",
    "                    steps_in_trap += 1\n",
    "                    if steps_in_trap > patience:\n",
    "                        return saved_trial_price * prob_booked\n",
    "                else:\n",
    "                    saved_trial_price = trial_price\n",
    "                    steps_in_trap = 0\n",
    "                prob_booked = prob_booked_trial\n",
    "                if (prob_booked > threshold and trial_price < 3 * current_price):\n",
    "                    pass\n",
    "                else:\n",
    "                    return saved_trial_price * prob_booked    \n",
    "            else:\n",
    "                steps_in_trap += 1\n",
    "                if steps_in_trap > patience:\n",
    "                    return saved_trial_price * prob_booked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use average of all probabilities of a property being avialable for booking predicted by the best version of each of the classifiers that I trained to get probability value smoothly varying the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "#                 pickle.load(open(\"gb_best.pkl\", 'rb')),\n",
    "#                pickle.load(open(\"rf_best.pkl\", 'rb')),\n",
    "#                 pickle.load(open(\"ada_best.pkl\", 'rb')),\n",
    "               nn_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating expected revenue at threshold: 0.025\n",
      "Expected revenue:  15684.28767090591\n",
      "Evaluating expected revenue at threshold: 0.05\n",
      "Expected revenue:  15680.40514813056\n",
      "Evaluating expected revenue at threshold: 0.075\n",
      "Expected revenue:  15681.124442491284\n",
      "Evaluating expected revenue at threshold: 0.1\n",
      "Expected revenue:  15689.653799255304\n",
      "Evaluating expected revenue at threshold: 0.125\n",
      "Expected revenue:  15697.528717478339\n",
      "Evaluating expected revenue at threshold: 0.15\n",
      "Expected revenue:  15720.602064757042\n",
      "Evaluating expected revenue at threshold: 0.175\n",
      "Expected revenue:  15719.68724362067\n",
      "Evaluating expected revenue at threshold: 0.2\n",
      "Expected revenue:  15740.617176625747\n",
      "Evaluating expected revenue at threshold: 0.225\n",
      "Expected revenue:  15759.09729224913\n",
      "Evaluating expected revenue at threshold: 0.25\n",
      "Expected revenue:  15776.240943320723\n",
      "Evaluating expected revenue at threshold: 0.275\n",
      "Expected revenue:  15785.582176043861\n",
      "Evaluating expected revenue at threshold: 0.3\n",
      "Expected revenue:  15822.59686044482\n",
      "Evaluating expected revenue at threshold: 0.325\n",
      "Expected revenue:  15856.125838881931\n",
      "Evaluating expected revenue at threshold: 0.35\n",
      "Expected revenue:  15850.189144476642\n",
      "Evaluating expected revenue at threshold: 0.375\n",
      "Expected revenue:  15839.626029009678\n",
      "Evaluating expected revenue at threshold: 0.4\n",
      "Expected revenue:  15857.032517561302\n",
      "Evaluating expected revenue at threshold: 0.425\n",
      "Expected revenue:  15848.617459019892\n",
      "Evaluating expected revenue at threshold: 0.45\n",
      "Expected revenue:  15867.95205970503\n",
      "Evaluating expected revenue at threshold: 0.475\n",
      "Expected revenue:  15905.258301673244\n",
      "Evaluating expected revenue at threshold: 0.5\n",
      "Expected revenue:  15921.857007186476\n",
      "Evaluating expected revenue at threshold: 0.525\n",
      "Expected revenue:  15917.742994346287\n",
      "Evaluating expected revenue at threshold: 0.55\n",
      "Expected revenue:  15890.106878031504\n",
      "Evaluating expected revenue at threshold: 0.575\n",
      "Expected revenue:  15853.80496621744\n",
      "Evaluating expected revenue at threshold: 0.6\n",
      "Expected revenue:  15795.716242347715\n",
      "Evaluating expected revenue at threshold: 0.625\n",
      "Expected revenue:  15727.438079629012\n",
      "Evaluating expected revenue at threshold: 0.65\n",
      "Expected revenue:  15545.711018297456\n",
      "Evaluating expected revenue at threshold: 0.675\n",
      "Expected revenue:  15206.360488921915\n",
      "Evaluating expected revenue at threshold: 0.7\n",
      "Expected revenue:  14836.495802905607\n",
      "Evaluating expected revenue at threshold: 0.725\n",
      "Expected revenue:  14468.328876231173\n",
      "Evaluating expected revenue at threshold: 0.75\n",
      "Expected revenue:  14030.249651229113\n",
      "Evaluating expected revenue at threshold: 0.775\n",
      "Expected revenue:  13687.08379426684\n",
      "Evaluating expected revenue at threshold: 0.8\n",
      "Expected revenue:  13011.126900324714\n",
      "Evaluating expected revenue at threshold: 0.825\n",
      "Expected revenue:  12135.779739338323\n",
      "Evaluating expected revenue at threshold: 0.85\n",
      "Expected revenue:  11689.711740315077\n",
      "Evaluating expected revenue at threshold: 0.875\n",
      "Expected revenue:  10674.416272868135\n",
      "Evaluating expected revenue at threshold: 0.9\n",
      "Expected revenue:  10211.066297483558\n",
      "Evaluating expected revenue at threshold: 0.925\n",
      "Expected revenue:  9639.182114682999\n",
      "Evaluating expected revenue at threshold: 0.95\n",
      "Expected revenue:  9138.938581299684\n",
      "Evaluating expected revenue at threshold: 0.975\n",
      "Expected revenue:  9108.211694350437\n"
     ]
    }
   ],
   "source": [
    "highest_revenue = -math.inf\n",
    "threshold_at_highest_revenue = None\n",
    "for threshold in thresholds:\n",
    "#     print(\"====================================================\\n\")    \n",
    "    print (\"Evaluating expected revenue at threshold: \" + str(threshold))\n",
    "#     print(\"====================================================\\n\")\n",
    "    revenue = 0\n",
    "    for index, row in X_test.sample(n=100, random_state=42).copy().iterrows():\n",
    "        revenue_at_tuned_price = calc_revenue_at_tuned_price(row, threshold, classifiers)\n",
    "#         print (\"threshold: \" + str(threshold))\n",
    "#         print (\"revenue at tuned price: \" + str(revenue_at_tuned_price))\n",
    "        revenue += revenue_at_tuned_price\n",
    "#         print (\"------------------------------------------------------------\")\n",
    "    print (\"Expected revenue: \", str(revenue))\n",
    "    if revenue > highest_revenue:\n",
    "        highest_revenue = revenue\n",
    "        threshold_at_highest_revenue = threshold\n",
    "    revenue_by_threshold.loc[threshold, \"revenue\"] = revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.025</th>\n",
       "      <td>15684.287671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050</th>\n",
       "      <td>15680.405148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.075</th>\n",
       "      <td>15681.124442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>15689.653799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.125</th>\n",
       "      <td>15697.528717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.150</th>\n",
       "      <td>15720.602065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.175</th>\n",
       "      <td>15719.687244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.200</th>\n",
       "      <td>15740.617177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.225</th>\n",
       "      <td>15759.097292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.250</th>\n",
       "      <td>15776.240943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.275</th>\n",
       "      <td>15785.582176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.300</th>\n",
       "      <td>15822.596860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.325</th>\n",
       "      <td>15856.125839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.350</th>\n",
       "      <td>15850.189144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.375</th>\n",
       "      <td>15839.626029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.400</th>\n",
       "      <td>15857.032518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.425</th>\n",
       "      <td>15848.617459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.450</th>\n",
       "      <td>15867.952060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.475</th>\n",
       "      <td>15905.258302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.500</th>\n",
       "      <td>15921.857007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.525</th>\n",
       "      <td>15917.742994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.550</th>\n",
       "      <td>15890.106878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.575</th>\n",
       "      <td>15853.804966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.600</th>\n",
       "      <td>15795.716242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.625</th>\n",
       "      <td>15727.438080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.650</th>\n",
       "      <td>15545.711018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.675</th>\n",
       "      <td>15206.360489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.700</th>\n",
       "      <td>14836.495803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.725</th>\n",
       "      <td>14468.328876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.750</th>\n",
       "      <td>14030.249651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.775</th>\n",
       "      <td>13687.083794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.800</th>\n",
       "      <td>13011.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.825</th>\n",
       "      <td>12135.779739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.850</th>\n",
       "      <td>11689.711740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.875</th>\n",
       "      <td>10674.416273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.900</th>\n",
       "      <td>10211.066297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.925</th>\n",
       "      <td>9639.182115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.950</th>\n",
       "      <td>9138.938581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.975</th>\n",
       "      <td>9108.211694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            revenue\n",
       "0.025  15684.287671\n",
       "0.050  15680.405148\n",
       "0.075  15681.124442\n",
       "0.100  15689.653799\n",
       "0.125  15697.528717\n",
       "0.150  15720.602065\n",
       "0.175  15719.687244\n",
       "0.200  15740.617177\n",
       "0.225  15759.097292\n",
       "0.250  15776.240943\n",
       "0.275  15785.582176\n",
       "0.300  15822.596860\n",
       "0.325  15856.125839\n",
       "0.350  15850.189144\n",
       "0.375  15839.626029\n",
       "0.400  15857.032518\n",
       "0.425  15848.617459\n",
       "0.450  15867.952060\n",
       "0.475  15905.258302\n",
       "0.500  15921.857007\n",
       "0.525  15917.742994\n",
       "0.550  15890.106878\n",
       "0.575  15853.804966\n",
       "0.600  15795.716242\n",
       "0.625  15727.438080\n",
       "0.650  15545.711018\n",
       "0.675  15206.360489\n",
       "0.700  14836.495803\n",
       "0.725  14468.328876\n",
       "0.750  14030.249651\n",
       "0.775  13687.083794\n",
       "0.800  13011.126900\n",
       "0.825  12135.779739\n",
       "0.850  11689.711740\n",
       "0.875  10674.416273\n",
       "0.900  10211.066297\n",
       "0.925   9639.182115\n",
       "0.950   9138.938581\n",
       "0.975   9108.211694"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenue_by_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15921.857007186476"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_at_highest_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XecVNX9//HXe3uhLCxLryJIseIq2Hv9JqKJSTQWNH7lG6OpX/OLpmk0zRiTfI0lakLUxNhSlKgRsWBvIIgUkZWOSu+wbPv8/rhncVxn2dllZmdn9/N8cB9z58wtnzMz7GfuPfeeIzPDOeecS4asdAfgnHOu/fCk4pxzLmk8qTjnnEsaTyrOOeeSxpOKc865pPGk4pxzLmk8qTjnnEsaTyrOOeeSxpOKc865pMlJdwCtrUePHjZ48OB0h+GccxllxowZa82srKnlOlxSGTx4MNOnT093GM45l1EkLU1kOT/95ZxzLmk8qTjnnEualCUVSZMkrZY0p0H51yUtkDRX0q9iyq+WVBFeOyWm/NRQViHpqpjyIZJel7RQ0oOS8lJVF+ecc4lJ5ZHK3cCpsQWSjgPGA/ub2Wjg16F8FHAOMDqsc5ukbEnZwK3AacAo4NywLMANwG/NbBiwAbgkhXVxzjmXgJQlFTN7AVjfoPgy4JdmtjMsszqUjwceMLOdZrYYqAAODVOFmS0ysyrgAWC8JAHHA38P698DnJmqujjnnEtMa7epDAeOCqetnpd0SCjvByyPWW5FKGusvBTYaGY1Dcqdc86lUWtfUpwDdAPGAYcAD0naC1CcZY34Sc92s3xckiYCEwEGDhzYzJCdc84lqrWTygrgnxaNYfyGpDqgRygfELNcf+CDMB+vfC1QIiknHK3ELv8pZnYncCdAeXm5j5/sWp2ZsXVnDZsra9hSWc2Wyho274get1RWs7myhp01deRkiews7Xr8eD6LorxsenbJp2fnAnp1yadTfg7RmWDn2o7WTiqPELWFTJM0HMgjShCTgb9J+g3QFxgGvEF0RDJM0hBgJVFj/pfNzCQ9B5xN1M4yAXi0levi3CdU1dSxcuMOlq3fzrJ121i6bjtL129n+frtLFu/ne1VtUndX1FeNj0759OzSwE9O+fTp2sBA7sXMSBM/bsVkp+TndR9OteUlCUVSfcDxwI9JK0ArgEmAZPCZcZVwIRw1DJX0kPAPKAGuNzMasN2rgCmANnAJDObG3bxPeABST8FZgJ/SlVdXOvbXFnNyg07omljNG3bWUNudha52SInOyuazxK5OVnkZIn83GwKcrIozMumMDeaCvKyKcjJpjAvmzozNu+Ijgo27agO89Vs3lHD5spqamrryMvJIj8nm/z6x9ws8rKzyM/NorbO2LS9mo07qtm4vZpNO6rYGPN8/bad1MUcB+fnZDGwexGDSos4bGgpfboW0KUgl84FuXQpzIkeC6LHzgU55OdkUWdQU1dHbZ1RU2fU1hq1ZtTWGVsqa1i9pZLVm3eyanMlq7d8/Dhn5SaemreKqpq6XfuXoHeXAgZ0i5LMXmXFHDighP36d6VLQW4aPlXXESj6m95xlJeXm3fTkn6V1bW7fsEvXRc9rtiwnRUhiWyprPnE8nk5WXTOz6G6to6aOqO6to7q2uR8d3OzRdfCXHKysqiqraOqpo6dNbWNbr84L5uSojy6FuZSUhRNXQvzKOuUx8DS4l2JpKxTPllZrXd6qq7OWLN1J8tijo6Wrd/OivXR0dNHmyt3LTu0rJgD+pdwwIBoGtmnsx/VuN2SNMPMyptarsP1/eWSo7bO2FoZ/cLfFH7xb99ZS2VNLZXV0R/lyuo6Kqtr2VlTx87qWtZurWLZ+m0sW7+dVZt3fmJ7xXnZDOheRL+SQg4d0p1+JYX061a467FH8af/QJtFv+Zrao2q2mifO6vr2FFdy46qWiqra9lR/fGjiJJHl8IcuhTkhvlc8nOy4rZN1NbZrgRTVVOHFK2fl9M2O6LIyhK9uhTQq0sBhwzu/qnXN22vZvbKjby9fCOzlm/ihYVr+efMlUCUWEf37crYvbozbkgp5YO70dmPZlwL+JFKB1YXTqls2F7Fhu1V4ZRO41N9w/LmHdVs2VnT9A5i5Odk0a0oj4GlRdEv+e5FDCyNTssM6l5E9+I8b3RuZWbGh5sqmb1iIzOXb+StpRuYtXwj1bVGlmDffl0ZO6Q7Y4eUcsiQ7nQt9CTTkSV6pOJJJUOYGduralm3tYp123aybmsV67dVsXbbTtaH+cqaqCFYiPAPSeERdlTVhgRSzYZtVWzcUU1tXeOff1FeNiXh13zXmMfOBdEv/S6FUZtA9JhLcX42BblRG0Z+btaux8aOBFzbs6OqlpnLNvDaonW8tng9s5ZtpKq2DgkO26uU68bvy949O6U7TJcGnlQa0dKk8uspC1izZSfZ2SJb8S/7RAIzDKh/Ww3DLLqJprbO2Fl/OijmtEp0eiicvqmp21UWe36/qqaOxv7+F+Zm0704j6K87LDvKAbCfus/4/ycbLoV59K9OI+Sojy6FeXSrSiPbkV5dC/Oo2tRlDS6hiTRVk/zuNZTWV3LzGUbeXXROu59dQnbq2r59onDufSoIeRk+/ejI/E2lSR7a9kG3l+zldo6qK2LGovrwhU6dWafaNRVzFECsOtIIScr6xNXE318lVEWeTlZlBTlhauPsnY95udkk5cTrdO5IIfuxXn06JRP9+IoEZR2yqMozz9GlxoFudkcNrSUw4aWcv64gfz4kbnc8OS7/GfOh/zq7P0Z0btLukN0bYwfqTjnmuXx2R/y40fnsLmymiuOG8bXjhtKrh+1tHuJHqn4N8E51yz/tX8fpn7nGE7btw+/ffo9zrjlZeas3JTusFwb4UnFOdds3YvzuPncg7jzgoNZt3Un4299mVueXZjusFwb4CfjnXMtdvLo3owdUsqPHp3Dr596j27FeZw3dlC6w3Jp5EnFObdHuhbl8tsvHcjmymqueXQuw3p25tAhn7750nUMfvrLObfHsrPE/51zEANLi7jsrzNYuXFHukNyaeJJxTmXFF0Lc7nrwnKqauqYeO90diS5V2aXGTypOOeSZmhZJ24+9yDmfbiZ7/79bTraLQvOk4pzLsmOG9GT/3fKCB6b/SG3P/9+usNxrcyTinMu6b56zF6ccUBfbpyygGfmr0p3OK4VeVJxziWdJG74/P6M7tuFbz4wi4rVW9IdkmslnlSccylRmJfNHReUU5CbxaX3zmDTjup0h+RagScV51zK9Csp5PbzD2bFhu184/6Z1O1mqAXXPnhScc6l1CGDu3PNZ0fz/HtrvOG+A/Ck4pxLufPGDuSzB/TlpqcW8Mbi9ekOx6VQypKKpEmSVkuaE1N2raSVkmaF6fSY166WVCFpgaRTYspPDWUVkq6KKR8i6XVJCyU9KCkvVXVxzu0ZSfz8rH0Z2L2Ib9w/k/XbqtIdkkuRVB6p3A2cGqf8t2Z2YJieAJA0CjgHGB3WuU1StqRs4FbgNGAUcG5YFuCGsK1hwAbgkhTWxTm3hzoX5HLLl8ewflsV33lolrevtFMpSypm9gKQ6HHueOABM9tpZouBCuDQMFWY2SIzqwIeAMYrGlLxeODvYf17gDOTWgHnXNLt268rP/rMSKYtWMOdLy5KdzguBdLRpnKFpNnh9Fi3UNYPWB6zzIpQ1lh5KbDRzGoalDvn2rjzxw3i9P16c+OUBcxY6u0r7U1rJ5XbgaHAgcCHwE2hXHGWtRaUxyVpoqTpkqavWbOmeRE755JKEr/8/P70Kynk63+byQZvX2lXWjWpmNkqM6s1szrgLqLTWxAdaQyIWbQ/8MFuytcCJZJyGpQ3tt87zazczMrLysqSUxnnXIt1Kcjlli8fxJqtO7nyYe94sj1p1aQiqU/M07OA+ivDJgPnSMqXNAQYBrwBvAkMC1d65RE15k+26Bv4HHB2WH8C8Ghr1ME5lxz79y/h+6eP5Jl3V/PHFxenOxyXJCkb+VHS/cCxQA9JK4BrgGMlHUh0qmoJ8D8AZjZX0kPAPKAGuNzMasN2rgCmANnAJDObG3bxPeABST8FZgJ/SlVdnHOpcdHhg3lt0TpuePJdDh7cjTEDuzW9kmvT1NEOO8vLy2369OnpDsM5F2zaXs1//f5FzOCJbxxF16LcdIfk4pA0w8zKm1rO76h3zqVV16Lo/pXVWyq50gf2ynieVJxzaXfggBKuOm0kU+etYtLLS9IdjtsDnlScc23CV44YzEmjevHL/8xn1vKN6Q7HtZAnFedcmyCJX599AD07F3D5fW+xabuPv5KJPKk459qMrkW53Hqet69kMk8qzrk2xdtXMpsnFedcm+PtK5nLk4pzrs3x9pXM5UnFOdcmRfevHMSqzd6+kkk8qTjn2qyDBnbjqtNGMHXeKv70kvcPlgk8qTjn2rRLjhzCiSN78asnF1Cxemu6w3FN8KTinGvTJPGLz+1HQW4W3//XOz4McRvnScU51+aVdc7n+6eP5I3F63l4xvKmV3Bp40nFOZcRvlg+gEOHdOdnj89nzZad6Q7HNcKTinMuI2RliZ+ftR+V1XVc/9i8dIfjGuFJxTmXMfbu2YnLjh3K5Lc/YNqC1ekOx8XhScU5l1G+dtxQ9ior5oePzGF7VU26w3ENeFJxzmWU/JxsfnHWfqzYsIP/e3phusNxDXhScc5lnLF7lfKl8gH88aXFzP1gU7rDcTE8qTjnMtLVp4+gW1EuV//zHWr93pU2I6GkIilbUl9JA+unBNaZJGm1pDlxXrtSkknqEZ5L0s2SKiTNljQmZtkJkhaGaUJM+cGS3gnr3CxJiVXZOdcelBTl8aPPjGL2ik3c++qSdIfjgiaTiqSvA6uAqcDjYXosgW3fDZwaZ3sDgJOAZTHFpwHDwjQRuD0s2x24BhgLHApcI6lbWOf2sGz9ep/al3OufTvjgL4cM7yMX09ZwAcbd6Q7HEdiRyrfBPYxs9Fmtl+Y9m9qJTN7AVgf56XfAv8PiD1eHQ/ca5HXgBJJfYBTgKlmtt7MNhAltlPDa13M7FWLui69Fzgzgbo459oRSfz0zH2pNeNHj8zxnozbgESSynIgKS1hks4AVprZ2w1e6hf2U29FKNtd+Yo45c65DmZA9yK+e8oInnl3NX94flG6w+nwchJYZhEwTdLjwK6+EczsN83ZkaQi4AfAyfFejlNmLShvbN8TiU6VMXBgk81BzrkM85UjBjNz2QZ+NeVdRvbpzLH79Ex3SB1WIkcqy4hOO+UBnWOm5hoKDAHelrQE6A+8Jak30ZHGgJhl+wMfNFHeP055XGZ2p5mVm1l5WVlZC0J3zrVlkvjV2fszoncXvnH/TJas3ZbukDqsJpOKmf3EzH4C/Aa4KeZ5s5jZO2bW08wGm9lgosQwxsw+AiYDF4arwMYBm8zsQ2AKcLKkbqGB/mRgSnhti6Rx4aqvC4FHmxuTc679KMrL4c4LDiYrS0z8y3S27vS77dMhkau/9pU0E5gDzJU0Q9LoBNa7H3gV2EfSCkmX7GbxJ4hOs1UAdwFfAzCz9cD1wJthui6UAVwG/DGs8z7wn6Zics61bwO6F3Hrl8dQsXor//vQLB97JQ3U1NUSkl4BfmBmz4XnxwI/N7PDUx9e8pWXl9v06dPTHYZzLoX++OIifvr4fP73pOF8/YRh6Q6nXZA0w8zKm1oukTaV4vqEAmBm04DiPYjNOedS6pIjh3DWQf34zdPv8fS8VekOp0NJJKkskvQjSYPD9ENgcaoDc865lqofgnh03y58+8FZPrZ9K0okqXwFKAP+CfwrzF+cyqCcc25PFeRmc8cF5eTlZDHxL9PZXFmd7pA6hESu/tpgZt8wszFmdpCZfTPc3e6cc21av5JCbjtvDMvWbefKhxrec+1SodGbHyX9zsy+JenfxLmx0MzOSGlkzjmXBGP3KuXbJw3nxikLmPvBJkb37ZrukNq13d1R/5fw+OvWCMQ551Ll/LGDuPmZhdz3+jJ+ftZ+6Q6nXWv09JeZzQizB5rZ87ETcGDrhOecc3uua1Eunz2gL4/MXMkWb1tJqUQa6ifEKbsoyXE451xKnT9uENuranlkVqM9Orkk2F2byrnAl4EhkibHvNQZWJfqwJxzLpkO6N+Vfft14b7XlnL+2IH4uH6psbs2lVeAD4EewE0x5VuA2akMyjnnkk0S540dxNX/fIcZSzdQPrh7ukNql3bXprI03D1/HvB6THvKfD7ZQ7BzzmWEMw7oS+f8HO57fVnTC7sWSaRN5SGgLuZ5LfBwasJxzrnUKc7P4awx/Xh89oes31aV7nDapUSSSo6Z7Xr3w3xe6kJyzrnUOW/sIKpq6/j7jOVNL+yaLZGksiYMAwyApPHA2tSF5JxzqbNP784cMrgb972+zLvGT4FEkspXge9LWi5pGfA94H9SG5ZzzqXO+eMGsXTddl6q8N/HydbkGPVm9j4wTlInovFXtqQ+LOecS51T9+1N9+I87nt9KUcP9yHGkymRkR97SfoT8LCZbZE0qolRHJ1zrk3Lz8nmC+X9eXr+aj7aVJnucNqVRE5/3U00Vnzf8Pw94FupCsg551rDeYcOorbOeOBNv7w4mRJJKj3MbNdlxWZWQ3RZsXPOZayBpUUcPbyMB95YTk1tXdMruIQkklS2SSoldH8vaRywKaVROedcKzh/7EA+2lzJM++uTnco7UYiSeU7wGRgqKSXgXuBrze1kqRJklZLmhNTdr2k2ZJmSXpKUt9QLkk3S6oIr4+JWWeCpIVhmhBTfrCkd8I6N8s78nHONdPxI3rSp2sBf31tabpDaTcSGfnxLeAY4HCiS4lHm1kifX/dDZzaoOxGM9vfzA4EHgN+HMpPA4aFaSJwO4Ck7sA1wFjgUOAaSd3COreHZevXa7gv55zbrZzsLM45ZCAvLlzLkrXb0h1Ou9BoUpF0fHj8HHAGsA8wHPispLMkHSMpu7H1zewFYH2Dss0xT4v5eETJ8cC9FnkNKJHUBzgFmGpm68MQxlOBU8NrXczsVTMzoqOnM5tVc+ecA750yACys8T9b3iDfTLs7j6VY4Bngc828nop8EPgpObsUNLPgAuJ2mWOC8X9gNg+E1aEst2Vr4hT7pxzzdK7awEnjezFwzNW8N1T9iEnO5FWAdeYRpOKmV0THi9ubJlw/0qzmNkPgB9Iuhq4guj0Vrz2EGtBeWNxTiQ6VcbAgQObG7Jzrp0786C+PDn3I95YvJ7D9+6R7nAyWiI3P3aV9BtJ08N0k6SuAGa2JzdB/g34fJhfAQyIea0/8EET5f3jlMdlZneaWbmZlZeV+d2zzrlPOnp4Gfk5WUyZ+1G6Q8l4iRznTSIamOuLYdoM/LklO5M0LObpGcC7YX4ycGG4CmwcsMnMPiS66fJkSd1CA/3JwJTw2hZJ48JVXxcCj7YkJuecK8rL4ejhZTw1bxVRM61rqSb7/gKGmtnnY57/RNKsplaSdD9wLNBD0gqi01ynS9qH6EbKpUSdVQI8AZwOVADbgYsBzGy9pOuBN8Ny15lZfeP/ZURXmBUC/wmTc861yMmjejF13ireWbmJ/fuXpDucjJVIUtkh6UgzewlA0hHAjqZWMrNz4xTHbYMJV3Bd3shrk4iOlhqWTwf2bSoO55xLxIkje5GdJabM/ciTyh5ItOv7WyUtkbQEuAXv+t451850K87j0MHdmTJ3VbpDyWi7TSqSsoB9zOwAYH9gfzM7KMGbH51zLqOcMroXFau38v6arekOJWPtNqmYWR3RZb+Y2eYGNy8651y7ctLo3gA85UcrLZbI6a+pkq6UNEBS9/op5ZE551wr61dSyH79uvqlxXsgkYb6r4TH2IZ0A/ZKfjjOOZdep4zuxa+feo9Vmyvp1aUg3eFknEQ6lBwSZ/KE4pxrl06pPwU2z0+BtUQid9QXSPqOpH9K+oekb0ny9O2ca5f27tmJIT2KecpPgbVIIm0q9wKjgd8TXU48CvhLKoNyzrl0kcTJo3vx6vvr2LS9Ot3hZJxEkso+ZnaJmT0XpolEXeA751y7dMro3tTUGc8t8BEhmyuRpDIz9McFgKSxwMupC8k559LrwP4l9Oyc71eBtUAiSWUs8ErMHfWvAseEoXz9JkjnXLuTlSVOGtWLaQvWUFldm+5wMkoilxT7ML3OuQ7nlNG9ue/1Zby0cC0njuqV7nAyRpNJxcyWtkYgzjnXlozbq5TOBTlMmfuRJ5Vm8HEznXMujrycLI4f0ZOn56+iprYu3eFkDE8qzjnXiFNG92bD9mreXLIh3aFkjISSiqRBkk4M84WSOqc2LOecS79jhpeRl5PFU/P8KrBEJXJH/aXA34E7QlF/4JFUBuWcc21BcX4ORw/rwVNzfZjhRCVypHI5cATR2PSY2UKgZyqDcs65tuLkUb1ZuXEHcz/wkT8SkUhS2WlmVfVPJOUQ9VLsnHPt3gkje5IlvC+wBCWSVJ6X9H2gUNJJwMPAv1MblnPOtQ2lnfI5ZHB3/jPnIz8FloBEkspVwBrgHaKx6Z8AftjUSpImSVotaU5M2Y2S3pU0W9K/JJXEvHa1pApJCySdElN+aiirkHRVTPkQSa9LWijpQUl5iVXZOeea5/MH92fh6q38a+bKdIfS5iUynkqdmd1lZl8ws7PDfCLp+m4+fTf+VGBfM9sfeA+4GkDSKOAcot6QTwVuk5QtKRu4FTiNqHfkc8OyADcAvzWzYcAG4JIEYnLOuWY7e0x/DhpYws8en8/G7VVNr9CBJXL112JJixpOTa1nZi8A6xuUPWVmNeHpa0RXkgGMBx4ws51mthioAA4NU4WZLQrtOg8A4yUJOJ7oqjSAe4Azm6ytc861QFaW+NmZ+7FxRzU3PLkg3eG0aYn0/VUeM18AfAFIxhj1XwEeDPP9iJJMvRWhDGB5g/KxQCmwMSZBxS7vnHNJN6pvF75yxGDuenExZx/cj4MHJePPYPuTyOmvdTHTSjP7HdFRQotJ+gFQA9xXXxRv1y0ob2x/EyVNlzR9zZo1zQ3XOecA+NaJw+nbtYAf/GsO1d51S1yJnP4aEzOVS/oq0OI76iVNAD4DnBfTNrMCGBCzWH/gg92UrwVKwuXNseVxmdmdZlZuZuVlZWUtDd0518EV5+dw7RmjefejLUx6aXG6w2mTEjn9dVPMfA2wBPhiS3Ym6VTge8AxZrY95qXJwN8k/QboCwwD3iA6IhkmaQiwkqgx/8tmZpKeA84mameZADzakpicc645Th7dm5NG9eJ3Ty/kv/bvQ/9uRekOqU1J5PTXcTHTSWZ2qZk12VIl6X6iAb32kbRC0iVEY9x3BqZKmiXpD2Efc4GHgHnAk8DlZlYb2kyuAKYA84GHwrIQJafvSKogamP5UzPr7pxzLXLtGaOR4NrJc/3elQbU1BsiKR/4PDCYmCMbM7supZGlSHl5uU2fPj3dYTjnMtxdLyziZ0/M544LDuaU0b3THU7KSZphZuVNLZfIzY+PEl3yWwNsi5mcc67DuuiIwYzo3ZlrJ89l686aplfoIBJpU+lvZj6ksHPOxcjNzuLnn9uPz9/+Cr+b+h4//MyoplfqABI5UnlF0n4pj8Q55zLMmIHdOPfQgfz5lSXM/WBTusNpExJJKkcCM0L/W7MlvSNpdqoDc865TPC9U0bQrSiXHz4yp+mFO4BETn+dlvIonHMuQ3UtyuVrx+7NdY/NY+m6bQwqLU53SGmVyCXFS4luQDw+zG9PZD3nnOsojhsRjVv4wsK1aY4k/RK5o/4aontCrg5FucBfUxmUc85lksGlRfTvVsiL73k3UIkccZwFnEG4jNjMPmAPumlxzrn2RhJHDevBq++vo6aD9wmWSFKpCn10GYCkjn3C0Dnn4jhqWBlbdtbw9oqN6Q4lrRJJKg9JuoOoA8dLgaeBu1IblnPOZZbDh5aSJXjhvY7drpJIQ/2viQbD+gewD/BjM/t9qgNzzrlMUlKUx379S3ipomMnlSYvKZb0beBhM5vaCvE451zGOnpYD26b9j6bdlTTtTA33eGkRSKnv7oAUyS9KOlySb1SHZRzzmWiI/fuQW2d8er769IdStokcvrrJ2Y2GricaKyT5yU9nfLInHMuwxw0sBvFedm8VNFxLy1uzk2Mq4GPgHVAz9SE45xzmSsvJ4vDhpbyYge+CTKRmx8vkzQNeAboAVxqZvunOjDnnMtER+7dg6XrtrNs3famF26HEun7axDwLTOblepgnHMu0x01vAyAFyvWcF7poDRH0/oSaVO5Cugk6WIASWVhzHjnnHMN7NWjmL5dC3ixg96v4n1/OedcEkVdtpTxyvtrO2SXLd73l3POJdlRw3uwubKG2Ss73sBdKev7S9IkSaslzYkp+4KkuZLqJJU3WP5qSRVhMLBTYspPDWUVkq6KKR8i6XVJCyU9KCkvkbiccy7VjhjaA4kOeQqspX1//TGB9e4GGo5tPwf4HPBCbKGkUcA5wOiwzm2SsiVlA7cSDRQ2Cjg3LAtwA/BbMxsGbAAuSSAm55xLuW7FeezXr2uHvF+lpX1/3ZzAei8A6xuUzTezBXEWHw88YGY7zWwxUAEcGqYKM1tkZlXAA8B4SQKOD3EB3AOc2VRMzjnXWo7cuwdvLdvIlsrqdIfSqhK6+dHMpprZd83sSuBZSeclOY5+wPKY5ytCWWPlpcBGM6tpUO6cc23CUcPKqK0zXlu0vumF25FGk4qkLqGd4xZJJytyBbAI+GKS41CcMmtBefyNSxMlTZc0fc2ajnc46pxrfWMGlVCUl82LCzvW35zd3fz4F6K2ileB/wa+C+QB41NwI+QKYEDM8/7AB2E+XvlaojaenHC0Erv8p5jZncCdAOXl5Y0mH+ecS5b8nGzGDune4bps2d3pr73M7CIzuwM4FygHPpOiO+snA+dIyg83Vg4D3gDeBIaFK73yiBrzJ4er0Z4Dzg7rTwAeTUFczjnXYkcNK2Px2m0sX99xumzZXVLZ1bpkZrXAYjPbkuiGJd1PdJSzj6QVki6RdJakFcBhwOOSpoTtzwUeAuYBTwKXm1ltOAq5ApgCzAceCstCdEPmdyRVELWx/CnR2JxzrjUcNawHQIcauEvRj/44L0i1hBseidowCoHtYd7MrEurRJjcW/+aAAAT0UlEQVRk5eXlNn369HSH4ZzrAMyMw37xLGMGlXDbeQenO5w9ImmGmZU3tVyjbSpmlp3ckJxzrmOJumzpwVPzVlFbZ2RnxbvGqH1pzngqzjnnmunIYT3YtKOadzpIly2eVJxzLoWO3Du0q3SQS4s9qTjnXAqVdspndN8uvNBBLi32pOKccyl2zPAyZizdQMXqrekOJeU8qTjnXIpdfMQQivKy+cm/59LYFbfthScV55xLsbLO+XznpOG8uHAtU+Z+lO5wUsqTinPOtYILxg1iRO/OXP/YfHZU1aY7nJTxpOKcc60gJzuLn5wxmpUbd3DbtIp0h5MynlScc66VjN2rlDMP7Msdzy9iydptTa+QgTypOOdcK7r69JHkZovrHpuX7lBSwpOKc861ol5dCvjWicN59t3VPD1vVbrDSTpPKs4518ouOmIwe/fsxE8em0tldftqtPek4pxzrSw3O4vrzhjN8vU7uOP5RekOJ6k8qTjnXBocvncP/mv/Ptw2raJdDeLlScU559LkB6ePJEvi+nbUaO9JxTnn0qRvSSFfP2Fvnpq3imkLVqc7nKTwpOKcc2l0yZFD2KtHMdf9ex41tXXpDmePeVJxzrk0ys/J5urTR7Jo7TYmv/1BusPZY55UnHMuzU4c2ZMRvTtz63MV1NVldi/GKUsqkiZJWi1pTkxZd0lTJS0Mj91CuSTdLKlC0mxJY2LWmRCWXyhpQkz5wZLeCevcLKn9D/7snGuXJHHF8Xvz/pptPJnhvRin8kjlbuDUBmVXAc+Y2TDgmfAc4DRgWJgmArdDlISAa4CxwKHANfWJKCwzMWa9hvtyzrmMcdq+fdirrJjfP1uR0WOupCypmNkLwPoGxeOBe8L8PcCZMeX3WuQ1oERSH+AUYKqZrTezDcBU4NTwWhcze9Wid//emG0551zGyc4SXzt2b+Z/uJln383cK8Fau02ll5l9CBAee4byfsDymOVWhLLdla+IU+6ccxlr/IF96d+tMKOPVtpKQ3289hBrQXn8jUsTJU2XNH3NmjUtDNE551IrNzuLrx4zlFnLN/LK++vSHU6LtHZSWRVOXREe64/xVgADYpbrD3zQRHn/OOVxmdmdZlZuZuVlZWV7XAnnnEuVsw/uT68u+dzybGYO5NXaSWUyUH8F1wTg0ZjyC8NVYOOATeH02BTgZEndQgP9ycCU8NoWSePCVV8XxmzLOecyVkFuNhOPHsqri9YxfUnDZum2L5WXFN8PvArsI2mFpEuAXwInSVoInBSeAzwBLAIqgLuArwGY2XrgeuDNMF0XygAuA/4Y1nkf+E+q6uKcc63p3EMH0L04j1uey7yjlZxUbdjMzm3kpRPiLGvA5Y1sZxIwKU75dGDfPYnROefaoqK8HC45cgg3TlnAnJWb2Ldf13SHlLC20lDvnHMuxgWHDaJzQU7Gta14UnHOuTaoS0EuFx8+mCfnfsR7q7akO5yEeVJxzrk26uIjhlCUl81tGdS24knFOefaqG7FeZw/bhCT3/6AJWu3pTuchHhScc65Nuy/jxpCTnYWf3j+/XSHkhBPKs4514b17FzAuYcM4O8zVrB0Xds/WvGk4pxzbdzlx+1NXk4Wv3pyQbpDaZInFeeca+N6ding0qP24vF3PmTG0g3pDme3PKk451wGmHj0XpR1zufnT8xv0z0Ye1JxzrkMUJyfw3dOGs6MpRuY0oZHh/Sk4pxzGeILB/dnWM9O3PDkAqpr69IdTlyeVJxzLkPkZGfx/dNHsnjtNv72+rJ0hxOXJxXnnMsgx+5TxuFDS/nd0++xubI63eF8iicV55zLIJL4/ukj2bC9mtuntb0bIj2pOOdchtm3X1fOOqgfk15azMqNO9Idzid4UnHOuQx05Sn7YMBNU9rWDZGeVJxzLgP1KynkK0cM4V+zVjJn5aZ0h7OLJxXnnMtQXztuKCWFufziP23nhkhPKs45l6G6FOTyzROG8XLFOqa9tybd4QApHKPeOedc6n157CDufmUJ3//nOxw3oid9uhTQp6SQPl0LwlRIYV52q8WTlqQi6ZvApYCAu8zsd5K6Aw8Cg4ElwBfNbIMkAf8HnA5sBy4ys7fCdiYAPwyb/amZ3dOqFXHOuTTLy8niV2cfwPWPzWPKnI9Yt63qU8uUFOXSu0sBD3/1MDoX5KY0nlZPKpL2JUoohwJVwJOSHg9lz5jZLyVdBVwFfA84DRgWprHA7cDYkISuAcoBA2ZImmxmbbsLT+ecS7JDh3Tn318/EoDK6lpWba7kg42VfLR5Bx9srOTDTTtYvXknnfJT/yc/HUcqI4HXzGw7gKTngbOA8cCxYZl7gGlESWU8cK9FrVCvSSqR1CcsO9XM1oftTAVOBe5vtZo451wbU5CbzaDSYgaVFqdl/+loqJ8DHC2pVFIR0WmtAUAvM/sQIDz2DMv3A5bHrL8ilDVW7pxzLk1a/UjFzOZLugGYCmwF3gZqdrOK4m1mN+Wf3oA0EZgIMHDgwGbF65xzLnFpuaTYzP5kZmPM7GhgPbAQWBVOaxEeV4fFVxAdydTrD3ywm/J4+7vTzMrNrLysrCy5lXHOObdLWpKKpJ7hcSDwOaJ2kMnAhLDIBODRMD8ZuFCRccCmcHpsCnCypG6SugEnhzLnnHNpkq77VP4hqRSoBi4Plw7/EnhI0iXAMuALYdkniNpdKoguKb4YwMzWS7oeeDMsd119o71zzrn0UFu5tb+1lJeX2/Tp09MdhnPOZRRJM8ysvKnlvJsW55xzSeNJxTnnXNJ0uNNfktYASxt5uQewthXDaWu8/l5/r3/H1VT9B5lZk5fPdriksjuSpidyzrC98vp7/b3+Xv893Y6f/nLOOZc0nlScc84ljSeVT7oz3QGkmde/Y/P6d2xJqb+3qTjnnEsaP1JxzjmXNB0yqUg6VdICSRVhQLCGr+dLejC8/rqkwa0fZeokUP/vSJonabakZyQNSkecqdJU/WOWO1uSSWpXVwQlUn9JXwzfgbmS/tbaMaZSAt//gZKekzQz/B84PR1xpoKkSZJWS5rTyOuSdHN4b2ZLGtPsnZhZh5qAbOB9YC8gj6jr/VENlvka8Icwfw7wYLrjbuX6HwcUhfnLOlr9w3KdgReA14DydMfdyp//MGAm0C0875nuuFu5/ncCl4X5UcCSdMedxPofDYwB5jTy+unAf4iGFhkHvN7cfXTEI5VDgQozW2RmVcADRKNLxhpPNPokwN+BEyTFG78lEzVZfzN7zsLInER/VPu3coyplMjnD3A98CugsjWDawWJ1P9S4FYLQ3Ob2Wraj0Tqb0CXMN+VRobUyERm9gLRcCON2TXSrpm9BtSPtJuwjphUEhkxctcyZlYDbAJKWyW61GvuiJmXEP1yaS+arL+kg4ABZvZYawbWShL5/IcDwyW9LOk1Sae2WnSpl0j9rwXOl7SCqJf0r7dOaG3CHo+om66u79MpkREjEx5VMgM1Z8TM84Fy4JiURtS6dlt/SVnAb4GLWiugVpbI559DdArsWKKj1Bcl7WtmG1McW2tIpP7nAneb2U2SDgP+Eupfl/rw0m6P//Z1xCOVREaM3LWMpByiQ+D2MlZLQiNmSjoR+AFwhpntbKXYWkNT9e8M7AtMk7SE6Lzy5HbUWJ/o9/9RM6s2s8XAAqIk0x4kUv9LgIcAzOxVoICoX6yOIOERdRvTEZPKm8AwSUMk5RE1xE9usEzsKJRnA89aaMVqB5qsfzj9cwdRQmlP59Ohifqb2SYz62Fmg81sMFGb0hlm1l4G4Unk+/8I0cUaSOpBdDpsUatGmTqJ1H8ZcAKApJFESWVNq0aZPo2NtJuwDnf6y8xqJF1BNPRwNjDJzOZKug6YbmaTgT8RHfJWEB2hnJO+iJMrwfrfCHQCHg7XJywzszPSFnQSJVj/divB+tcP1T0PqAW+a2br0hd18iRY//8F7pL0baJTPxe1lx+Vku4nOq3ZI7QZXQPkApjZH2hkpN1m7aOdvFfOOefagI54+ss551yKeFJxzjmXNJ5UnHPOJY0nFeecc0njScU551zSeFJpoySVSpoVpo8krQzzG8Olnsne37GSmtUtiaRp8W4KlHSRpFuasZ18SU+H+n2pOTEksO1mxRLW2dpI+XXhptBP1F3SE5JKwvS1FsR4Y+gN+MbmrttWhO/P4c1c50xJo5q5zt2Szm5edLvWHdxY77wt2NaScA9PS9Zt9ncyk3S4+1QyRbgv4EAASdcCW83s14q64W/yj7+knNBvWSY4CMg1swNbsnJr1dXMftxI+ekhjsFEPVzf1sxN/w9QlmjPBS2pb+gQVanoaiT0OnEssBV4pRmrnkn0XU76jySXPn6kkpmyJd0Vft0+JakQdv16/rmk54FvSiqT9A9Jb4bpiLDcMTFHQTMldQ7b7STp75LelXRf+EOEpBPCcu8oGo8hv2FAki6W9F7Y9xHxgpbUXdIjisZpeE3S/pJ6An8FDgzxDG2wzjRJv5P0iqQ5kg4N5ddKulPSU8C9kgok/TnEOFPScTGbGSDpSUVjaFwTs+1HJM0I7+PEBvu9SdJbisaTKQtlcX8lx/xq/SUwNNTjRkl/kTQ+Zrn7JJ3RYN3JQDHwuqQvSRoU9lk/ls3AmH3/RtJzwA0NtnGRpEcb1jH8Mp8v6TbgrfA+nBveozmSbojZxtZG6jw0bHeGpBcljYgTz4PAV4Fvh7ofJWmxpNywbJfwHuXG7O9w4AzgxvrPXdKB4XsxW9K/JHVr+F4HJ4ZY3pP0mbC97PCevxnW/59G1s2RdE9Y5u+SisL6cb/jTX33JRWG9+fS8Px8SW+EOt0hKTuUN/n/o91Id//+PiU0BsK1wJVhfjBQAxwYnj8EnB/mpwG3xaz3N+DIMD8QmB/m/w0cEeY7ER2xHkvUG3N/oh8brwJHEnVRsRwYHpa/F/hWzP7KgT5EXVuUEY1R8TJwS5x6/B64JswfD8wK88cCjzVS92nAXWH+aMI4EOE9mQEUhuf/C/w5zI8I8RQQdQz5IVEv04XAHML4KED38FhfXhqeG3BemP9xfV2Au4GzY+se5pcQ9Q01mJhxKog64nwkzHcFFgM5ceq4NWb+38CEMP+VmPXvJvpVnx1n/bh1DPHUAePCcn1jPqcc4FngzCbq/AwwLMyPJeqy6FPxEPMdDc//HLPticBNceLe9X6G57OBY8L8dcDvGlnnSaLv6DCivqoKwj5+GJbJB6YDQxqsOzjUs/67Pwm4kka+442Vx3zmg4GngQtD2cjw+eWG57cBF5Lg/4/2MvmRSmZabGazwvwMoi93vQdj5k8EbpE0i6hPny6KjkpeBn4j6RtAiX18KuUNM1th0SmSWWG7+4T9vReWuYfoj3usscA0M1tj0RgVDxLfkcBfAMzsWaBUUtcE6nt/WOeFUIeSUD7ZzHbE2fa7wFKiPqsApprZurDsP8OyAN+Q9DZR/14D+LjTxLqYOvw1ZvlmMbPngb3D0di5wD+s6dNWhxH9GCDUJ3bfD5tZbSPrNVbHpRaNiwFwCB9/TjXAfXz8WX6qzpI6AYcTddczi6g/uNixNXYXzx/5uIuPi4mSTKPC96AkvGcQ/3tW7yEzqzOzhUR9ko0ATibqs2oW8DpRgo3XCeZyM3s5tp40/h1v6rv/KNEPmXvD8xOAg4E3QxwnEA0Gluj/j3bB21QyU+y591qiX6f1tsXMZwGHxfzhrfdLSY8T9fHzmkLjc5zt5hC/K+x4Eunvp6Xdajdcpv55bF13F+en1pd0LFHSPczMtkuaRvTLNJH1m+MvwHlE/cd9pQXrx+57W6NL7fl7FG97WcBGa7ytq9F4zOzlcPrtGKKjmaQ0kMfE1vC5gK+b2ZQWrhtPU+/Xy8Bpkv5m0aGJgHvM7OpPbEQ6M85+2y0/UmnfngKuqH8iqb7hf6iZvWNmNxCdJhixm228CwyWtHd4fgHwfINlXgeOVXTFWi7whUa29QLRH1jCH/W1ZrY5gXp8KaxzJFGvqZua2PZwotN9C8JrJylqzykkahx+meh01IaQUEYQdXFfL4uod2qALwMvJRAjwBairvNj3U10KgUzm5vANl7h4w5Mz2vGvuPVsaHXgWMk9Qjn+s/l48/yU3UOn81iSV+AXeOXH9DI/uPV/V6io8zGjlJ2rRM+0w2Sjgqvxfue1fuCpCxF7W97EX3OU4DLYtpxhksqjrPuQEVjpEBU/5do/Dve1Hf/x8A6Pr4w4xng7HBkWt+GOIjE/3+0C55U2rdvAOWhUXIeUWMqwLdCQ+3bwA52M7KjmVUSnb54WNI7RKdJ/tBgmQ+Jzqm/SnSO+a1GNndtfTxEjdoTGlmuoQ2SXgn7vaSRZW4juoDhHaLTCxfZx1dTvUR0xDCL6BTUdKLz8jkhluuJToHV2waMljSDqO3nukSCtOiKvZfDe3tjKFsFzKeJ0z8xvgFcHOK6APhmguvFq2PD+D4ErgaeIxqb/S0zezS83FidzwMuCd+VucQfehmitoSz6hvqQ9l9QDfC6cs4HgC+GxrChxJ9H24MdT+Qxt/3BUR/3P8DfDV8R/9IdBXZW4ouG76D+Gdi5gMTwj66A7c39h1P5LtPaHuR9Cszmwf8EHgqbH8q0KcZ/z/aBe+l2LVp4bTUlfH+SGaCcHXRO8CYRo6wkrGPi4guGriiqWV3s42tZtYpeVGBoivlxpvZBcncrmvbvE3FuRQJbVWTgN+kKqG0VZJ+D5xG1G7nOhA/UnHOOZc03qbinHMuaTypOOecSxpPKs4555LGk4pzzrmk8aTinHMuaTypOOecS5r/D3K6U3dIfcJJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(revenue_by_threshold.index, revenue_by_threshold)\n",
    "plt.ylabel('Revenue projection')\n",
    "plt.xlabel('Threshold of probability for property to be booked')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating percentage of change in expected revenue before and after tuning price using occupancy prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_revenue_original_price = 0\n",
    "expected_revenue_modified_price = 0\n",
    "\n",
    "for index, row in X_test.sample(n=100, random_state=42).copy().iterrows():\n",
    "    def calc_average_prob_booked(row):        \n",
    "        df = pd.DataFrame(data=row.to_dict(), index=[0])\n",
    "        sum_prob_booked = 0\n",
    "        for clf in classifiers:\n",
    "            sum_prob_booked += clf.predict_proba(df)[0][0]\n",
    "        return sum_prob_booked/len(classifiers)\n",
    "    prob_booked = calc_average_prob_booked(row)\n",
    "    expected_revenue_original_price += prob_booked    * row[\"price_calendar\"]\n",
    "    revenue_at_tuned_price = calc_revenue_at_tuned_price(row, threshold_at_highest_revenue, classifiers)\n",
    "    expected_revenue_modified_price += revenue_at_tuned_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentag_change = 100*(expected_revenue_modified_price - expected_revenue_original_price)/expected_revenue_original_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.53088419507424"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentag_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
